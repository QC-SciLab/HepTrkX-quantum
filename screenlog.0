Account expires						: Sep 01, 2020
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ [K[ctuysuz@culture-plate-sm HepTrkX-quantum]$ [K[ctuysuz@culture-plate-sm HepTrkX-quantum]$ /storage/group/gpu/software/gpuservers/singularity/run.sh /storage/group/gpu/software/singu larity/ibanks/over.simg 
+ binding=
+ '[' -d /nfshome ']'
+ '[' -d /etc/grid-security/letsencrypt ']'
+ binding=' -B /etc/grid-security/letsencrypt'
+ '[' -d /data ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data'
+ '[' -d /bigdata ']'
+ '[' -d /imdata ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata'
+ '[' -d /mnt/hadoop ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop'
+ '[' -d /storage ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage'
+ ex=
+ img=/storage/group/gpu/software/singularity/ibanks/edge.simg
+ '[' '!' -z /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ '[' -r /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ echo using /storage/group/gpu/software/singularity/ibanks/over.simg as image
using /storage/group/gpu/software/singularity/ibanks/over.simg as image
+ img=/storage/group/gpu/software/singularity/ibanks/over.simg
+ ex=
+ set -x
+ '[' '!' -z '' ']'
+ XDG_RUNTIME_DIR=
+ JUPYTER_DATA_DIR=/tmp/ctuysuz/jupyter
+ LC_ALL=C
+ singularity shell -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage --nv /storage/group/gpu/software/singularity/ibanks/over.simg
[33mWARNING:[0m underlay of /etc/grid-security/letsencrypt required more than 50 (125) bind mounts
Singularity> python3 train.py configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 1
n_train: 1400
lr: 0.01
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-04-24 04:43:03.877971 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_params_NN.csv
2020-04-24 04:43:03.878238 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/summary.csv
2020-04-24 04:43:03.878531 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_params_IN.csv
2020-04-24 04:43:03.878892 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_grads_EN.csv
2020-04-24 04:43:03.879149 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_grads_NN.csv
2020-04-24 04:43:03.879379 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_params_EN.csv
2020-04-24 04:43:03.879620 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_grads_IN.csv
2020-04-24 04:43:03.879856 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_validation.csv
2020-04-24 04:43:03.880093 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_loss.csv
Starting testing the validation set with 1 subgraphs!
2020-04-24 04:43:11.488285: Validation Test:  Loss: 0.6368,  Acc: 65.1724, AUC: 0.6744, Precision: 0.7925 -- Elapsed: 0m6s
2020-04-24 04:43:11.488364: Training is starting!
^[:wqAccount expires						: Sep 01, 2020
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ /storage/group/gpu/software/gpuservers/singularity/run.sh /storage/group/gpu/software/singu larity/ibanks/over.simg 
+ binding=
+ '[' -d /nfshome ']'
+ '[' -d /etc/grid-security/letsencrypt ']'
+ binding=' -B /etc/grid-security/letsencrypt'
+ '[' -d /data ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data'
+ '[' -d /bigdata ']'
+ '[' -d /imdata ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata'
+ '[' -d /mnt/hadoop ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop'
+ '[' -d /storage ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage'
+ ex=
+ img=/storage/group/gpu/software/singularity/ibanks/edge.simg
+ '[' '!' -z /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ '[' -r /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ echo using /storage/group/gpu/software/singularity/ibanks/over.simg as image
using /storage/group/gpu/software/singularity/ibanks/over.simg as image
+ img=/storage/group/gpu/software/singularity/ibanks/over.simg
+ ex=
+ set -x
+ '[' '!' -z '' ']'
+ XDG_RUNTIME_DIR=
+ JUPYTER_DATA_DIR=/tmp/ctuysuz/jupyter
+ LC_ALL=C
+ singularity shell -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage --nv /storage/group/gpu/software/singularity/ibanks/over.simg
[33mWARNING:[0m underlay of /etc/grid-security/letsencrypt required more than 50 (125) bind mounts
Singularity> python3 train.py configs/q comparisons/qgnn/iteration_comparison/                     learning_rate_comparison/lr_2e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_2e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.02
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_2e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
^CTraceback (most recent call last):
  File "train.py", line 80, in <module>
    loss, grads = gradient(graph_array,labels)
  File "train.py", line 22, in gradient
    return loss, tape.gradient(loss,block.trainable_variables)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", line 1029, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/usr/local/lib64/python3.6/site-packages/tensorflow_core/python/ops/custom_gradient.py", line 439, in actual_grad_fn
    input_grads = grad_fn(*result_grads)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/interfaces/tf.py", line 92, in grad
    jacobian = qnode.jacobian(args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 918, in jacobian
    grad[:, i] = self._pd_analytic(flat_params, k, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 1047, in _pd_analytic
    y1 = np.asarray(self.evaluate(shift_p2, **circuit_kwargs))
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/autograd/tracer.py", line 48, in f_wrapped
    return f_raw(*args, **kwargs)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/qnode.py", line 682, in evaluate
    ret = self.device.execute(self.circuit.operations, self.circuit.observables, self.variable_deps)
  File "/storage/user/ctuysuz/.local/lib/python3.6/site-packages/pennylane/_device.py", line 179, in execute
    results.append(self.expval(obs.name, obs.wires, obs.parameters))
  File "/usr/local/lib/python3.6/site-packages/pennylane_qulacs/qulacs_device.py", line 196, in expval
    expectation = inner_product(bra, self._state)
KeyboardInterrupt
Singularity> vim configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml 
[?1049h[?1h=[1;73r[34l[34h[?25h[23m[24m[0m[H[J[?25l[73;1H"configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml" 18L, 423C[1;1H[33m  1 [0m[36mtrain_dir[0m[35m    :[0m [31m'data/graph_data/train'[0m
[33m  2 [0m[36mvalid_dir[0m[35m    :[0m [31m'data/graph_data/valid'[0m
[33m  3 [0m[36mparam_dir[0m[35m    :[0m [31m'params/'[0m
[33m  4 [0m[36mlog_dir[0m[35m      :[0m [31m'logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/'[0m
[33m  5 [0m[36mrun_type[0m[35m     :[0m [31m'new_run'[0m
[33m  6 [0m[36mgpu[0m[35m          :[0m [31m'7'[0m
[33m  7 [0m[36mn_files[0m[35m      :[0m [31m1600[0m
[33m  8 [0m[36mn_valid[0m[35m      :[0m [31m1[0m
[33m  9 [0m[36mn_train[0m[35m      :[0m [31m1400[0m
[33m 10 [0m[36mlr[0m[35m           :[0m [31m0.01[0m
[33m 11 [0m[36mn_iters[0m[35m      :[0m [31m2[0m
[33m 12 [0m[36mn_epoch[0m[35m      :[0m [31m1[0m
[33m 13 [0m[36mTEST_every[0m[35m   :[0m [31m50[0m
[33m 14 [0m[36mhid_dim[0m[35m      :[0m [31m1[0m
[33m 15 [0m[36mnetwork[0m[35m      :[0m [31m'QGNN'[0m
[33m 16 [0m[36mn_thread[0m[35m     :[0m [31m4[0m
[33m 17 [0m[36mlog_verbosity[0m[35m:[0m [31m2[0m
[33m 18 [0m
[1m[34m~                                                                                                                                      [20;1H~                                                                                                                                      [21;1H~                                                                                                                                      [22;1H~                                                                                                                                      [23;1H~                                                                                                                                      [24;1H~                                                                                                                                      [25;1H~                                                                                                                                      [26;1H~                                                                                                                                      [27;1H~                                                                                                                                      [28;1H~                                                                                                                                      [29;1H~                                                                                                                                      [30;1H~                                                                                                                                      [31;1H~                                                                                                                                      [32;1H~                                                                                                                                      [33;1H~                                                                                                                                      [34;1H~                                                                                                                                      [35;1H~                                                                                                                                      [36;1H~                                                                                                                                      [37;1H~                                                                                                                                      [38;1H~                                                                                                                                      [39;1H~                                                                                                                                      [40;1H~                                                                                                                                      [41;1H~                                                                                                                                      [42;1H~                                                                                                                                      [43;1H~                                                                                                                                      [44;1H~                                                                                                                                      [45;1H~                                                                                                                                      [46;1H~                                                                                                                                      [47;1H~                                                                                                                                      [48;1H~                                                                                                                                      [49;1H~                                                                                                                                      [50;1H~                                                                                                                                      [51;1H~                                                                                                                                      [52;1H~                                                                                                                                      [53;1H~                                                                                                                                      [54;1H~                                                                                                                                      [55;1H~                                                                                                                                      [56;1H~                                                                                                                                      [57;1H~                                                                                                                                      [58;1H~                                                                                                                                      [59;1H~                                                                                                                                      [60;1H~                                                                                                                                      [61;1H~                                                                                                                                      [62;1H~                                                                                                                                      [63;1H~                                                                                                                                      [64;1H~                                                                                                                                      [65;1H~                                                                                                                                      [66;1H~                                                                                                                                      [67;1H~                                                                                                                                      [68;1H~                                                                                                                                      [69;1H~                                                                                                                                      [70;1H~                                                                                                                                      [71;1H~                                                                                                                                      [72;1H~                                                                                                                                      [0m[73;118H8,1[11CAll[8;5H[34h[?25h[?25l[73;118H9[9;5H[34h[?25h[?25l[73;120H2[9;6H[34h[?25h[?25l[73;118H8[8;6H[34h[?25h[?25l[73;120H3[8;7H[34h[?25h[?25l[73;120H4[8;8H[34h[?25h[?25l[73;120H5[8;9H[34h[?25h[?25l[73;120H6[8;10H[34h[?25h[?25l[73;120H7[8;11H[34h[?25h[?25l[73;120H8[8;12H[34h[?25h[?25l[73;120H9[8;13H[34h[?25h[?25l[73;120H10[8;14H[34h[?25h[?25l[73;121H1[8;15H[34h[?25h[?25l[73;121H2[8;16H[34h[?25h[?25l[73;121H3[8;17H[34h[?25h[?25l[73;121H4[8;18H[34h[?25h[?25l[73;121H5[8;19H[34h[?25h[?25l[73;121H6[8;20H[34h[?25h[?25l[73;1H[1m-- INSERT --[0m[73;13H[K[73;118H8,16[10CAll[8;20H[34h[?25h[?25l[73;121H7[8;21H[34h[?25h[?25l[8;20H[K[73;121H6[8;20H[34h[?25h[?25l[31m2[0m[73;121H7[8;21H[34h[?25h[?25l[31m0[0m[73;121H8[8;22H[34h[?25h[?25l[31m0[0m[73;121H9[8;23H[34h[?25h[73;1H[K[?25l[73;118H8,18[10CAll[8;22H[34h[?25h[?25l[73;118H[K[73;1H:[34h[?25hwq[?25l"configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml" 18L, 425C written
[?1l>[34h[?25h[?1049lSingularity> vim configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml Singularity> [13@python3 train.py configs/comparisons/qgnn/learning_rate_comparison/lr_1e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.01
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
2020-04-24 04:44:32.964701 Deleted old log: logs/comparisons/qgnn/learning_rate_comparison/lr_1e-2/log_validation.csv
Starting testing the validation set with 200 subgraphs!
Account expires						: Sep 01, 2020
kctuysuz@culture-plate-sm:~/HepTrkX-quantum\[ctuysuz@culture-plate-sm HepTrkX-quantum]$ /storage/group/gpu/software/gpuservers/singularity/run.sh /storage/group/gpu/software/singu larity/ibanks/over.simg 
+ binding=
+ '[' -d /nfshome ']'
+ '[' -d /etc/grid-security/letsencrypt ']'
+ binding=' -B /etc/grid-security/letsencrypt'
+ '[' -d /data ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data'
+ '[' -d /bigdata ']'
+ '[' -d /imdata ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata'
+ '[' -d /mnt/hadoop ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop'
+ '[' -d /storage ']'
+ binding=' -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage'
+ ex=
+ img=/storage/group/gpu/software/singularity/ibanks/edge.simg
+ '[' '!' -z /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ '[' -r /storage/group/gpu/software/singularity/ibanks/over.simg ']'
+ echo using /storage/group/gpu/software/singularity/ibanks/over.simg as image
using /storage/group/gpu/software/singularity/ibanks/over.simg as image
+ img=/storage/group/gpu/software/singularity/ibanks/over.simg
+ ex=
+ set -x
+ '[' '!' -z '' ']'
+ XDG_RUNTIME_DIR=
+ JUPYTER_DATA_DIR=/tmp/ctuysuz/jupyter
+ LC_ALL=C
+ singularity shell -B /etc/grid-security/letsencrypt -B /data -B /imdata -B /mnt/hadoop -B /storage --nv /storage/group/gpu/software/singularity/ibanks/over.simg
[33mWARNING:[0m underlay of /etc/grid-security/letsencrypt required more than 50 (125) bind mounts
Singularity> python3 train.py configs/comparisons/qgnn/L 
dimension_comparison/     iteration_comparison/     learning_rate_comparison/ 
Singularity> python3 train.py configs/comparisons/qgnn/ler arning_rate_comparison/lr_2e-2.yaml           3e-2.yaml 
/storage/user/ctuysuz/HepTrkX-quantum/tools/tools.py:100: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(ymlfile)
Printing configs: 
train_dir: data/graph_data/train
valid_dir: data/graph_data/valid
param_dir: params/
log_dir: logs/comparisons/qgnn/learning_rate_comparison/lr_3e-2/
run_type: new_run
gpu: 7
n_files: 1600
n_valid: 200
n_train: 1400
lr: 0.03
n_iters: 2
n_epoch: 1
TEST_every: 50
hid_dim: 1
network: QGNN
n_thread: 4
log_verbosity: 2
Log dir: logs/comparisons/qgnn/learning_rate_comparison/lr_3e-2/
Training data input dir: data/graph_data/train
Validation data input dir: data/graph_data/train
Starting testing the validation set with 200 subgraphs!
2020-04-24 05:18:39.134229: Validation Test:  Loss: 0.7859,  Acc: 48.4221, AUC: 0.4778, Precision: 0.5517 -- Elapsed: 34m45s
2020-04-24 05:18:39.134316: Training is starting!
2020-04-24 05:19:27.408295: Validation Test:  Loss: 0.7393,  Acc: 47.1277, AUC: 0.4563, Precision: 0.5364 -- Elapsed: 34m53s
2020-04-24 05:19:27.408377: Training is starting!
2020-04-24 05:20:46.835994: Validation Test:  Loss: 0.7514,  Acc: 52.4676, AUC: 0.5324, Precision: 0.5954 -- Elapsed: 35m26s
2020-04-24 05:20:46.836079: Training is starting!
2020-04-24 05:23:36.751152: Epoch: 1, Batch: 1, Loss: 0.8325, Elapsed: 2m49s
2020-04-24 05:24:13.949662: Epoch: 1, Batch: 1, Loss: 0.7195, Elapsed: 4m46s
2020-04-24 05:25:48.522282: Epoch: 1, Batch: 1, Loss: 0.7716, Elapsed: 7m9s
2020-04-24 05:29:35.562508: Epoch: 1, Batch: 2, Loss: 0.7191, Elapsed: 5m58s
2020-04-24 05:30:29.543834: Epoch: 1, Batch: 2, Loss: 0.7318, Elapsed: 6m15s
2020-04-24 05:31:20.192177: Epoch: 1, Batch: 2, Loss: 0.7396, Elapsed: 5m31s
2020-04-24 05:33:10.035429: Epoch: 1, Batch: 3, Loss: 0.8022, Elapsed: 3m34s
2020-04-24 05:34:34.144971: Epoch: 1, Batch: 3, Loss: 0.7407, Elapsed: 3m13s
2020-04-24 05:35:45.392533: Epoch: 1, Batch: 3, Loss: 0.7082, Elapsed: 5m15s
2020-04-24 05:36:57.330429: Epoch: 1, Batch: 4, Loss: 0.7492, Elapsed: 3m47s
2020-04-24 05:39:45.315336: Epoch: 1, Batch: 4, Loss: 0.7519, Elapsed: 5m11s
2020-04-24 05:41:45.839605: Epoch: 1, Batch: 5, Loss: 0.7393, Elapsed: 4m48s
2020-04-24 05:44:23.164240: Epoch: 1, Batch: 5, Loss: 0.7876, Elapsed: 4m37s
2020-04-24 05:45:53.490824: Epoch: 1, Batch: 4, Loss: 0.7346, Elapsed: 10m8s
2020-04-24 05:46:47.509320: Epoch: 1, Batch: 6, Loss: 0.7463, Elapsed: 5m1s
2020-04-24 05:49:18.875460: Epoch: 1, Batch: 6, Loss: 0.7802, Elapsed: 4m55s
2020-04-24 05:52:16.052237: Epoch: 1, Batch: 7, Loss: 0.7022, Elapsed: 5m28s
2020-04-24 05:52:21.394841: Epoch: 1, Batch: 5, Loss: 0.7210, Elapsed: 6m27s
2020-04-24 05:55:25.679334: Epoch: 1, Batch: 7, Loss: 0.8148, Elapsed: 6m6s
2020-04-24 05:58:56.216621: Epoch: 1, Batch: 6, Loss: 0.6972, Elapsed: 6m34s
2020-04-24 05:59:27.339924: Epoch: 1, Batch: 8, Loss: 0.6953, Elapsed: 7m11s
2020-04-24 06:04:22.108126: Epoch: 1, Batch: 7, Loss: 0.7250, Elapsed: 5m25s
2020-04-24 06:05:05.377672: Epoch: 1, Batch: 9, Loss: 0.7035, Elapsed: 5m38s
2020-04-24 06:06:01.117626: Epoch: 1, Batch: 8, Loss: 0.7889, Elapsed: 10m35s
2020-04-24 06:09:39.660706: Epoch: 1, Batch: 10, Loss: 0.7375, Elapsed: 4m34s
2020-04-24 06:10:33.315886: Epoch: 1, Batch: 8, Loss: 0.6934, Elapsed: 6m11s
2020-04-24 06:12:02.325033: Epoch: 1, Batch: 9, Loss: 0.7224, Elapsed: 6m1s
2020-04-24 06:15:30.290418: Epoch: 1, Batch: 9, Loss: 0.7321, Elapsed: 4m56s
2020-04-24 06:16:23.123921: Epoch: 1, Batch: 10, Loss: 0.7924, Elapsed: 4m20s
2020-04-24 06:17:09.613944: Epoch: 1, Batch: 11, Loss: 0.7334, Elapsed: 7m29s
2020-04-24 06:22:24.229452: Epoch: 1, Batch: 11, Loss: 0.7488, Elapsed: 6m1s
2020-04-24 06:23:29.488972: Epoch: 1, Batch: 10, Loss: 0.7429, Elapsed: 7m59s
2020-04-24 06:26:21.262633: Epoch: 1, Batch: 12, Loss: 0.6928, Elapsed: 9m11s
2020-04-24 06:26:50.664630: Epoch: 1, Batch: 12, Loss: 0.7210, Elapsed: 4m26s
2020-04-24 06:32:22.998624: Epoch: 1, Batch: 11, Loss: 0.7369, Elapsed: 8m53s
2020-04-24 06:32:58.568499: Epoch: 1, Batch: 13, Loss: 0.7076, Elapsed: 6m37s
2020-04-24 06:35:02.763729: Epoch: 1, Batch: 13, Loss: 0.7802, Elapsed: 8m12s
2020-04-24 06:38:49.481900: Epoch: 1, Batch: 14, Loss: 0.6997, Elapsed: 3m46s
2020-04-24 06:39:02.095125: Epoch: 1, Batch: 14, Loss: 0.7100, Elapsed: 6m3s
2020-04-24 06:40:02.262929: Epoch: 1, Batch: 12, Loss: 0.7393, Elapsed: 7m39s
2020-04-24 06:43:09.550632: Epoch: 1, Batch: 15, Loss: 0.7260, Elapsed: 4m20s
2020-04-24 06:46:12.560796: Epoch: 1, Batch: 15, Loss: 0.6981, Elapsed: 7m10s
2020-04-24 06:48:09.798018: Epoch: 1, Batch: 13, Loss: 0.7313, Elapsed: 8m7s
2020-04-24 06:48:16.262655: Epoch: 1, Batch: 16, Loss: 0.7265, Elapsed: 5m6s
2020-04-24 06:51:41.376603: Epoch: 1, Batch: 17, Loss: 0.7300, Elapsed: 3m25s
2020-04-24 06:53:00.370470: Epoch: 1, Batch: 16, Loss: 0.6909, Elapsed: 6m47s
2020-04-24 06:56:49.833264: Epoch: 1, Batch: 17, Loss: 0.7061, Elapsed: 3m49s
2020-04-24 06:57:35.199187: Epoch: 1, Batch: 18, Loss: 0.7112, Elapsed: 5m53s
2020-04-24 06:57:47.081821: Epoch: 1, Batch: 14, Loss: 0.7105, Elapsed: 9m37s
2020-04-24 07:02:04.374583: Epoch: 1, Batch: 19, Loss: 0.6935, Elapsed: 4m29s
2020-04-24 07:06:01.836560: Epoch: 1, Batch: 18, Loss: 0.6963, Elapsed: 9m11s
2020-04-24 07:06:27.251073: Epoch: 1, Batch: 20, Loss: 0.7127, Elapsed: 4m22s
2020-04-24 07:09:18.201636: Epoch: 1, Batch: 15, Loss: 0.7257, Elapsed: 11m31s
2020-04-24 07:09:22.175540: Epoch: 1, Batch: 19, Loss: 0.7202, Elapsed: 3m20s
2020-04-24 07:15:27.148055: Epoch: 1, Batch: 21, Loss: 0.7112, Elapsed: 8m59s
2020-04-24 07:15:30.597570: Epoch: 1, Batch: 20, Loss: 0.6990, Elapsed: 6m8s
2020-04-24 07:15:50.288806: Epoch: 1, Batch: 16, Loss: 0.7048, Elapsed: 6m32s
2020-04-24 07:21:23.886103: Epoch: 1, Batch: 22, Loss: 0.7502, Elapsed: 5m56s
2020-04-24 07:24:39.953115: Epoch: 1, Batch: 17, Loss: 0.7265, Elapsed: 8m49s
2020-04-24 07:25:10.108890: Epoch: 1, Batch: 21, Loss: 0.7189, Elapsed: 9m39s
2020-04-24 07:28:00.329942: Epoch: 1, Batch: 23, Loss: 0.7168, Elapsed: 6m36s
2020-04-24 07:30:40.828874: Epoch: 1, Batch: 18, Loss: 0.7151, Elapsed: 6m0s
2020-04-24 07:30:46.245976: Epoch: 1, Batch: 22, Loss: 0.7083, Elapsed: 5m36s
2020-04-24 07:33:38.078365: Epoch: 1, Batch: 24, Loss: 0.7074, Elapsed: 5m37s
2020-04-24 07:35:33.061941: Epoch: 1, Batch: 19, Loss: 0.6681, Elapsed: 4m52s
2020-04-24 07:40:23.045561: Epoch: 1, Batch: 25, Loss: 0.7033, Elapsed: 6m44s
2020-04-24 07:41:32.971985: Epoch: 1, Batch: 23, Loss: 0.6972, Elapsed: 10m46s
2020-04-24 07:46:00.526206: Epoch: 1, Batch: 20, Loss: 0.7317, Elapsed: 10m27s
2020-04-24 07:49:10.942664: Epoch: 1, Batch: 26, Loss: 0.7191, Elapsed: 8m47s
2020-04-24 07:50:58.987772: Epoch: 1, Batch: 24, Loss: 0.6937, Elapsed: 9m25s
2020-04-24 07:52:34.249037: Epoch: 1, Batch: 21, Loss: 0.6700, Elapsed: 6m33s
2020-04-24 07:54:18.495511: Epoch: 1, Batch: 27, Loss: 0.6894, Elapsed: 5m7s
2020-04-24 07:54:38.033267: Epoch: 1, Batch: 25, Loss: 0.6996, Elapsed: 3m39s
2020-04-24 07:57:21.504765: Epoch: 1, Batch: 22, Loss: 0.7418, Elapsed: 4m47s
2020-04-24 08:00:51.851217: Epoch: 1, Batch: 28, Loss: 0.7199, Elapsed: 6m33s
2020-04-24 08:00:58.405077: Epoch: 1, Batch: 26, Loss: 0.6993, Elapsed: 6m20s
2020-04-24 08:03:49.669814: Epoch: 1, Batch: 23, Loss: 0.7201, Elapsed: 6m28s
2020-04-24 08:06:06.554272: Epoch: 1, Batch: 27, Loss: 0.6992, Elapsed: 5m8s
2020-04-24 08:07:29.183826: Epoch: 1, Batch: 29, Loss: 0.7053, Elapsed: 6m37s
2020-04-24 08:09:05.420587: Epoch: 1, Batch: 24, Loss: 0.6739, Elapsed: 5m15s
2020-04-24 08:11:13.185557: Epoch: 1, Batch: 30, Loss: 0.6795, Elapsed: 3m43s
2020-04-24 08:12:39.248919: Epoch: 1, Batch: 28, Loss: 0.6888, Elapsed: 6m32s
2020-04-24 08:15:42.396396: Epoch: 1, Batch: 31, Loss: 0.6902, Elapsed: 4m29s
2020-04-24 08:17:42.163330: Epoch: 1, Batch: 25, Loss: 0.7233, Elapsed: 8m36s
2020-04-24 08:18:25.901840: Epoch: 1, Batch: 29, Loss: 0.6960, Elapsed: 5m46s
2020-04-24 08:20:09.599885: Epoch: 1, Batch: 32, Loss: 0.7199, Elapsed: 4m27s
2020-04-24 08:25:50.449464: Epoch: 1, Batch: 26, Loss: 0.7116, Elapsed: 8m8s
2020-04-24 08:26:12.832519: Epoch: 1, Batch: 33, Loss: 0.7055, Elapsed: 6m3s
2020-04-24 08:26:22.038931: Epoch: 1, Batch: 30, Loss: 0.7062, Elapsed: 7m56s
2020-04-24 08:32:12.927776: Epoch: 1, Batch: 34, Loss: 0.6741, Elapsed: 6m0s
2020-04-24 08:32:42.197440: Epoch: 1, Batch: 27, Loss: 0.7212, Elapsed: 6m51s
2020-04-24 08:32:41.685541: Epoch: 1, Batch: 31, Loss: 0.6932, Elapsed: 6m19s
2020-04-24 08:38:58.836369: Epoch: 1, Batch: 32, Loss: 0.6973, Elapsed: 6m17s
2020-04-24 08:40:11.085555: Epoch: 1, Batch: 28, Loss: 0.6893, Elapsed: 7m28s
2020-04-24 08:42:28.268846: Epoch: 1, Batch: 35, Loss: 0.7045, Elapsed: 10m15s
2020-04-24 08:44:43.517387: Epoch: 1, Batch: 33, Loss: 0.6942, Elapsed: 5m44s
2020-04-24 08:46:03.857336: Epoch: 1, Batch: 29, Loss: 0.6822, Elapsed: 5m52s
2020-04-24 08:49:35.009485: Epoch: 1, Batch: 36, Loss: 0.7036, Elapsed: 7m6s
2020-04-24 08:52:47.695837: Epoch: 1, Batch: 30, Loss: 0.7076, Elapsed: 6m43s
2020-04-24 08:53:52.227237: Epoch: 1, Batch: 34, Loss: 0.6884, Elapsed: 9m8s
2020-04-24 08:55:33.873304: Epoch: 1, Batch: 37, Loss: 0.7045, Elapsed: 5m58s
2020-04-24 08:56:20.057113: Epoch: 1, Batch: 31, Loss: 0.7288, Elapsed: 3m32s
2020-04-24 08:58:45.820614: Epoch: 1, Batch: 35, Loss: 0.6963, Elapsed: 4m53s
2020-04-24 09:01:05.624758: Epoch: 1, Batch: 38, Loss: 0.7024, Elapsed: 5m31s
2020-04-24 09:03:29.403001: Epoch: 1, Batch: 36, Loss: 0.6898, Elapsed: 4m43s
2020-04-24 09:03:39.460672: Epoch: 1, Batch: 32, Loss: 0.6816, Elapsed: 7m19s
2020-04-24 09:08:53.978821: Epoch: 1, Batch: 37, Loss: 0.6920, Elapsed: 5m24s
2020-04-24 09:09:24.240232: Epoch: 1, Batch: 33, Loss: 0.7288, Elapsed: 5m44s
2020-04-24 09:10:29.061931: Epoch: 1, Batch: 39, Loss: 0.6901, Elapsed: 9m23s
2020-04-24 09:14:27.530504: Epoch: 1, Batch: 38, Loss: 0.6812, Elapsed: 5m33s
2020-04-24 09:15:15.102801: Epoch: 1, Batch: 40, Loss: 0.6833, Elapsed: 4m46s
2020-04-24 09:16:55.210416: Epoch: 1, Batch: 34, Loss: 0.7049, Elapsed: 7m30s
2020-04-24 09:19:28.131107: Epoch: 1, Batch: 41, Loss: 0.6807, Elapsed: 4m13s
2020-04-24 09:21:19.362821: Epoch: 1, Batch: 39, Loss: 0.7037, Elapsed: 6m51s
2020-04-24 09:24:09.437149: Epoch: 1, Batch: 35, Loss: 0.7001, Elapsed: 7m14s
2020-04-24 09:25:25.555088: Epoch: 1, Batch: 42, Loss: 0.6793, Elapsed: 5m57s
2020-04-24 09:29:32.316394: Epoch: 1, Batch: 40, Loss: 0.6924, Elapsed: 8m12s
2020-04-24 09:34:02.453948: Epoch: 1, Batch: 43, Loss: 0.7075, Elapsed: 8m36s
2020-04-24 09:35:19.893048: Epoch: 1, Batch: 41, Loss: 0.7015, Elapsed: 5m47s
2020-04-24 09:35:30.582745: Epoch: 1, Batch: 36, Loss: 0.6947, Elapsed: 11m21s
2020-04-24 09:38:32.765430: Epoch: 1, Batch: 37, Loss: 0.6369, Elapsed: 3m2s
2020-04-24 09:39:45.440852: Epoch: 1, Batch: 42, Loss: 0.6788, Elapsed: 4m25s
2020-04-24 09:41:15.797415: Epoch: 1, Batch: 44, Loss: 0.6705, Elapsed: 7m13s
2020-04-24 09:45:32.827462: Epoch: 1, Batch: 38, Loss: 0.7079, Elapsed: 7m0s
2020-04-24 09:46:07.767445: Epoch: 1, Batch: 43, Loss: 0.6863, Elapsed: 6m22s
2020-04-24 09:51:52.579106: Epoch: 1, Batch: 39, Loss: 0.6737, Elapsed: 6m19s
2020-04-24 09:53:58.348862: Epoch: 1, Batch: 45, Loss: 0.6921, Elapsed: 12m42s
2020-04-24 09:54:25.567115: Epoch: 1, Batch: 44, Loss: 0.6970, Elapsed: 8m17s
2020-04-24 09:56:23.460840: Epoch: 1, Batch: 40, Loss: 0.6651, Elapsed: 4m30s
2020-04-24 10:00:31.157230: Epoch: 1, Batch: 45, Loss: 0.6920, Elapsed: 6m5s
2020-04-24 10:04:09.614923: Epoch: 1, Batch: 46, Loss: 0.6761, Elapsed: 10m11s
2020-04-24 10:05:37.604908: Epoch: 1, Batch: 46, Loss: 0.6948, Elapsed: 5m6s
2020-04-24 10:07:26.159179: Epoch: 1, Batch: 41, Loss: 0.7162, Elapsed: 11m2s
2020-04-24 10:09:33.225393: Epoch: 1, Batch: 47, Loss: 0.6794, Elapsed: 5m23s
2020-04-24 10:12:24.564130: Epoch: 1, Batch: 42, Loss: 0.7056, Elapsed: 4m58s
2020-04-24 10:13:52.595236: Epoch: 1, Batch: 47, Loss: 0.6803, Elapsed: 8m14s
2020-04-24 10:18:37.294174: Epoch: 1, Batch: 48, Loss: 0.6908, Elapsed: 4m44s
2020-04-24 10:18:52.703281: Epoch: 1, Batch: 43, Loss: 0.6679, Elapsed: 6m28s
2020-04-24 10:20:32.984648: Epoch: 1, Batch: 48, Loss: 0.6841, Elapsed: 10m59s
2020-04-24 10:22:14.382632: Epoch: 1, Batch: 49, Loss: 0.6783, Elapsed: 3m37s
2020-04-24 10:26:19.499398: Epoch: 1, Batch: 44, Loss: 0.6816, Elapsed: 7m26s
2020-04-24 10:27:29.816572: Epoch: 1, Batch: 50, Loss: 0.6904, Elapsed: 5m15s
Starting testing the validation set with 200 subgraphs!
2020-04-24 10:29:27.093129: Epoch: 1, Batch: 49, Loss: 0.6947, Elapsed: 8m54s
2020-04-24 10:30:23.502302: Epoch: 1, Batch: 45, Loss: 0.6549, Elapsed: 4m3s
2020-04-24 10:34:37.727365: Epoch: 1, Batch: 50, Loss: 0.6749, Elapsed: 5m10s
Starting testing the validation set with 200 subgraphs!
2020-04-24 10:38:17.646062: Epoch: 1, Batch: 46, Loss: 0.7025, Elapsed: 7m54s
2020-04-24 10:44:27.785661: Epoch: 1, Batch: 47, Loss: 0.7040, Elapsed: 6m10s
2020-04-24 10:49:04.721385: Epoch: 1, Batch: 48, Loss: 0.6877, Elapsed: 4m36s
2020-04-24 10:55:26.268082: Epoch: 1, Batch: 49, Loss: 0.7023, Elapsed: 6m21s
2020-04-24 11:01:06.785581: Epoch: 1, Batch: 50, Loss: 0.7004, Elapsed: 5m40s
Starting testing the validation set with 200 subgraphs!
2020-04-24 11:06:13.530044: Validation Test:  Loss: 0.6877,  Acc: 54.3950, AUC: 0.5649, Precision: 0.6232 -- Elapsed: 38m43s
2020-04-24 11:11:27.928462: Epoch: 1, Batch: 51, Loss: 0.6963, Elapsed: 5m14s
2020-04-24 11:12:22.122519: Validation Test:  Loss: 0.6858,  Acc: 56.3284, AUC: 0.5826, Precision: 0.6404 -- Elapsed: 37m44s
2020-04-24 11:16:48.434059: Epoch: 1, Batch: 51, Loss: 0.6833, Elapsed: 4m26s
2020-04-24 11:17:06.969930: Epoch: 1, Batch: 52, Loss: 0.6927, Elapsed: 5m39s
2020-04-24 11:21:54.661281: Epoch: 1, Batch: 53, Loss: 0.6883, Elapsed: 4m47s
2020-04-24 11:22:05.723791: Epoch: 1, Batch: 52, Loss: 0.6940, Elapsed: 5m17s
2020-04-24 11:25:41.305009: Epoch: 1, Batch: 53, Loss: 0.6840, Elapsed: 3m35s
2020-04-24 11:28:19.315428: Epoch: 1, Batch: 54, Loss: 0.6794, Elapsed: 6m24s
2020-04-24 11:30:31.024560: Epoch: 1, Batch: 54, Loss: 0.6803, Elapsed: 4m49s
2020-04-24 11:33:18.183241: Epoch: 1, Batch: 55, Loss: 0.6815, Elapsed: 4m58s
2020-04-24 11:38:29.688820: Epoch: 1, Batch: 55, Loss: 0.6900, Elapsed: 7m58s
2020-04-24 11:39:20.733463: Epoch: 1, Batch: 56, Loss: 0.6855, Elapsed: 6m2s
2020-04-24 11:41:26.779917: Validation Test:  Loss: 0.6954,  Acc: 51.8133, AUC: 0.5151, Precision: 0.6008 -- Elapsed: 40m19s
2020-04-24 11:43:26.250904: Epoch: 1, Batch: 57, Loss: 0.6741, Elapsed: 4m5s
2020-04-24 11:44:29.922712: Epoch: 1, Batch: 56, Loss: 0.6847, Elapsed: 6m0s
2020-04-24 11:46:54.688216: Epoch: 1, Batch: 51, Loss: 0.7098, Elapsed: 5m27s
2020-04-24 11:50:01.816316: Epoch: 1, Batch: 57, Loss: 0.6858, Elapsed: 5m31s
2020-04-24 11:50:10.689146: Epoch: 1, Batch: 58, Loss: 0.6804, Elapsed: 6m44s
2020-04-24 11:53:23.041312: Epoch: 1, Batch: 52, Loss: 0.7044, Elapsed: 6m28s
2020-04-24 11:55:04.549527: Epoch: 1, Batch: 58, Loss: 0.6802, Elapsed: 5m2s
